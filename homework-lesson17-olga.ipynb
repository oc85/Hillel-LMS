{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Rainforcement learning\n\n\nusing on of the models defined in HuggingFace make a short scene \n\n\n### References\n- https://huggingface.co/learn/deep-rl-course/unit4/hands-on\n- https://huggingface.co/learn/deep-rl-course/unit2/hands-on?fw=pt\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Create a virtual display","metadata":{}},{"cell_type":"code","source":"%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1\n\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:52:31.377555Z","iopub.execute_input":"2023-11-09T09:52:31.377908Z","iopub.status.idle":"2023-11-09T09:53:04.068385Z","shell.execute_reply.started":"2023-11-09T09:52:31.377873Z","shell.execute_reply":"2023-11-09T09:53:04.067378Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install the dependencies ","metadata":{}},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:53:04.070128Z","iopub.execute_input":"2023-11-09T09:53:04.070425Z","iopub.status.idle":"2023-11-09T09:53:27.127642Z","shell.execute_reply.started":"2023-11-09T09:53:04.070400Z","shell.execute_reply":"2023-11-09T09:53:27.126514Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import the packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:53:48.601499Z","iopub.execute_input":"2023-11-09T09:53:48.602442Z","iopub.status.idle":"2023-11-09T09:53:48.610163Z","shell.execute_reply.started":"2023-11-09T09:53:48.602407Z","shell.execute_reply":"2023-11-09T09:53:48.609047Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:53:55.512966Z","iopub.execute_input":"2023-11-09T09:53:55.513488Z","iopub.status.idle":"2023-11-09T09:53:55.554336Z","shell.execute_reply.started":"2023-11-09T09:53:55.513450Z","shell.execute_reply":"2023-11-09T09:53:55.552759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model: Playing CartPole-v1","metadata":{}},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\n# Create the env\nenv = gym.make(env_id)\n\n# Create the evaluation env\neval_env = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n\n\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:54:00.358293Z","iopub.execute_input":"2023-11-09T09:54:00.359019Z","iopub.status.idle":"2023-11-09T09:54:00.375115Z","shell.execute_reply.started":"2023-11-09T09:54:00.358974Z","shell.execute_reply":"2023-11-09T09:54:00.374260Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build the Reinforce Architecture\n    Two fully connected layers (fc1 and fc2).\n    To use ReLU as activation function of fc1\n    To use Softmax to output a probability distribution over actions","metadata":{}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        #state = torch.as_tensor(np.array(state).astype('float').unsqueeze(0)).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:54:03.755482Z","iopub.execute_input":"2023-11-09T09:54:03.756197Z","iopub.status.idle":"2023-11-09T09:54:03.764145Z","shell.execute_reply.started":"2023-11-09T09:54:03.756163Z","shell.execute_reply":"2023-11-09T09:54:03.762982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"debug_policy = Policy(s_size, a_size, 64).to(device)\ndebug_policy.act(env.reset())","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:54:08.489856Z","iopub.execute_input":"2023-11-09T09:54:08.490208Z","iopub.status.idle":"2023-11-09T09:54:11.614168Z","shell.execute_reply.started":"2023-11-09T09:54:08.490181Z","shell.execute_reply":"2023-11-09T09:54:11.612810Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()\n        # Line 4 of pseudocode\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break \n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t) \n        n_steps = len(rewards) \n        # Compute the discounted returns at each timestep,\n        # as \n        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n        #\n        # In O(N) time, where N is the number of time steps\n        # (this definition of the discounted return G_t follows the definition of this quantity \n        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n        # G_t = r_(t+1) + r_(t+2) + ...\n        \n        # Given this formulation, the returns at each timestep t can be computed \n        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n        # G_t = r_(t+1) + gamma*G_(t+1)\n        # G_(t-1) = r_t + gamma* G_t\n        # (this follows a dynamic programming approach, with which we memorize solutions in order \n        # to avoid computing them multiple times)\n        \n        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n        \n        \n        ## Given the above, we calculate the returns at timestep t as: \n        #               gamma[t] * return[t] + reward[t]\n        #\n        ## We compute this starting from the last timestep to the first, in order\n        ## to employ the formula presented above and avoid redundant computations that would be needed \n        ## if we were to do it from first to last.\n        \n        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n        ## a normal python list would instead require O(N) to do this.\n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)>0 else 0)\n            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n            \n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n        ## eps is the smallest representable float, which is \n        # added to the standard deviation of the returns to avoid numerical instabilities        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n        \n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        # Line 8: PyTorch prefers gradient descent \n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n        \n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n        \n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:54:13.513208Z","iopub.execute_input":"2023-11-09T09:54:13.513954Z","iopub.status.idle":"2023-11-09T09:54:13.526588Z","shell.execute_reply.started":"2023-11-09T09:54:13.513921Z","shell.execute_reply":"2023-11-09T09:54:13.525334Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train model","metadata":{}},{"cell_type":"code","source":"cartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:54:16.963143Z","iopub.execute_input":"2023-11-09T09:54:16.963670Z","iopub.status.idle":"2023-11-09T09:54:16.969744Z","shell.execute_reply.started":"2023-11-09T09:54:16.963622Z","shell.execute_reply":"2023-11-09T09:54:16.968595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create policy and place it to the device\n\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-09T09:54:18.997506Z","iopub.execute_input":"2023-11-09T09:54:18.998187Z","iopub.status.idle":"2023-11-09T09:54:19.004179Z","shell.execute_reply.started":"2023-11-09T09:54:18.998154Z","shell.execute_reply":"2023-11-09T09:54:19.003242Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"], \n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"], \n                   100)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:31:11.975375Z","iopub.execute_input":"2023-11-09T10:31:11.976117Z","iopub.status.idle":"2023-11-09T10:31:12.199570Z","shell.execute_reply.started":"2023-11-09T10:31:11.976083Z","shell.execute_reply":"2023-11-09T10:31:12.198258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade numpy==1.16.4","metadata":{"execution":{"iopub.status.busy":"2023-11-09T10:01:18.070923Z","iopub.execute_input":"2023-11-09T10:01:18.071317Z","iopub.status.idle":"2023-11-09T10:02:28.057137Z","shell.execute_reply.started":"2023-11-09T10:01:18.071287Z","shell.execute_reply":"2023-11-09T10:02:28.055915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.from_numpy(np.asarray(x))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The Reinforce agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        for step in range(max_steps):\n            action, _ = policy.act(state)\n            new_state, reward, done, info = env.step(action)\n            total_rewards_ep += reward\n\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_agent(\n    eval_env, cartpole_hyperparameters[\"max_t\"], cartpole_hyperparameters[\"n_evaluation_episodes\"], cartpole_policy\n)","metadata":{},"outputs":[],"execution_count":null}]}