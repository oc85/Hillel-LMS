{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Transformers\n\n\nusing model from HuggingFace make a translator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"#### Transformers provides the following tasks out of the box: Sentiment analysis: is a text positive or negative?\n\nText generation (in English): provide a prompt and the model will generate what follows.\nName entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\nQuestion answering: provide the model with some context and a question, extract the answer from the context.\nFilling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\nSummarization: generate a summary of a long text.\nLanguage Translation: translate a text into another language.\nFeature extraction: return a tensor representation of the text.","metadata":{}},{"cell_type":"markdown","source":"### The Transformers package contains over 30 pre-trained models and 100 languages, along with eight major architectures for natural language understanding (NLU) and natural language generation (NLG):\n\n    BERT (from Google);\n    GPT (from OpenAI);\n    GPT-2 (from OpenAI);\n    Transformer-XL (from Google/CMU);\n    XLNet (from Google/CMU);\n    XLM (from Facebook);\n    RoBERTa (from Facebook);\n    DistilBERT (from Hugging Face).\n","metadata":{}},{"cell_type":"markdown","source":"### References:\n- https://www.kdnuggets.com/2021/02/hugging-face-transformer-basics.html\n- https://www.kaggle.com/code/scratchpad/notebook24768ac198/edit\n- https://huggingface.co/languages\n- https://github.com/Helsinki-NLP/UkrainianLT\n- https://huggingface.co/docs/transformers/model_doc/t5","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom transformers import pipeline # Text Summarization","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:03:12.202986Z","iopub.execute_input":"2023-11-07T14:03:12.203341Z","iopub.status.idle":"2023-11-07T14:03:12.249280Z","shell.execute_reply.started":"2023-11-07T14:03:12.203313Z","shell.execute_reply":"2023-11-07T14:03:12.247423Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline \u001b[38;5;66;03m# Text Summarization\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneratio\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'T5ForConditionalGeneratio' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'T5ForConditionalGeneratio' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)","output_type":"error"}],"execution_count":19},{"cell_type":"markdown","source":"### Install Transformer","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:29:58.842681Z","iopub.execute_input":"2023-11-07T13:29:58.843066Z","iopub.status.idle":"2023-11-07T13:30:09.376800Z","shell.execute_reply.started":"2023-11-07T13:29:58.843038Z","shell.execute_reply":"2023-11-07T13:30:09.376007Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Model: GPT2\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n\n\n### T5\n\nT5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German: …, for summarization: summarize: …. T5 comes in different sizes: small, base, large, 3b and 11b","metadata":{}},{"cell_type":"markdown","source":"### Different functions","metadata":{}},{"cell_type":"code","source":"generator = pipeline('text-generation', model='gpt2')\nset_seed(42)\n\ngenerator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)\n#generator(\"This year we are waiting\", max_length=10, num_return_sequences=5)\n#generator(\"Image processing has a great potential for \", max_length=10, num_return_sequences=5)\n\n##############################################################################################\n\n# Sentiment analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('The secret of getting ahead is getting started.')\n\n##############################################################################################\n\n# Allocate a pipeline for question-answering\nquestion_answerer = pipeline('question-answering')\nquestion_answerer({\n    'question': 'What is the Newtons third law of motion?',\n    'context': 'Newton’s third law of motion states that, \"For every action there is equal and opposite reaction\"'})\n\n# Question-Answer\n##############################################################################################\nnlp = pipeline(\"question-answering\")\n\ncontext = r\"\"\"\nComputers can do lots of jobs. They can do maths, store information, or play music. You can use a computer to write or to play games. What do you know about the history of computers?\nThe first computers were very big. They were the size of a room! They were so big that people didn't have them at home. Early computers could also only do simple maths, like a calculator. In the 1930s Alan Turing had the idea for a computer you could program to do different things.\nIn 1958 Jack Kilby invented the microchip. Microchips are tiny but can store lots of information. They helped make computers smaller. In the 1970s computers were smaller and cheaper so people started to use them at home. In the 1980s computer games were very popular. Lots of people bought computers just to play games.\nIn 1989 Tim Berners-Lee invented the World Wide Web, which is a way to organise information on the internet. Now people all over the world can look for and share information on websites.\nToday people can use smartphones to play games, email and go on the internet. In the past a simple computer was the size of a room. Now it can go in your pocket!\nFun facts\nMore than 5 billion people use the internet!\nMore than 300 billion emails are sent every day!\nThe first computer mouse was made of wood!\n\"\"\"\n\n#Question 1\nresult = nlp(question=\"Is early computers were very big and could do maths?\", context=context)\nprint(f\"Answer 1: '{result['answer']}'\")\n\n#Question 2\nresult = nlp(question=\"Is Alan Turing had the idea for a computer you could program?\", context=context)\nprint(f\"Answer 2: '{result['answer']}'\")\n\n##############################################################################################\n# Text prediction\nunmasker = pipeline('fill-mask', model='bert-base-cased')\nunmasker(\"Hello, My name is [MASK].\")","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:32:36.791681Z","iopub.execute_input":"2023-11-07T13:32:36.792067Z","iopub.status.idle":"2023-11-07T13:32:48.142385Z","shell.execute_reply.started":"2023-11-07T13:32:36.792039Z","shell.execute_reply":"2023-11-07T13:32:48.141745Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b2b8a2b765495fa0740f6c19060a17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36259a2b8eef4007bb75dbae67b95a52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"023ceca08b1b45c8beac98a8e5eaff40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a78dd4fcdae047dda0e2dced53ec8bf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e19c447517bb4c2ca6d7c1e21e7f7591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bdec111f659435591021cd687e6729b"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': \"Hello, I like to play cricket, but what I do in that sport is different—I've been here for over 12 years, and I've heard what people were saying: 'Well, your father should have been playing cricket in this country by now'. The first thing to know is that people\"},\n {'generated_text': 'Hello, I like to play cricket, sometimes, but they\\'re the same rules,\" said his nephew, who has been playing cricket for many years and has not been seen on television.\\n\\n\\'One bad cricket match might not be enough\\'\\n\\nA year ago the retired ex-colleg'},\n {'generated_text': \"Hello, I like to play cricket, but I've never been in a place that can get me started as a bat pro, so that has to be my goal.\\n\\nWith the Ashes there is just too much talk about the World Cup qualifiers and how they don't get any more important.\"},\n {'generated_text': 'Hello, I like to play cricket, not cricket.\"\\n\\nAnd you\\'re not the smartest.\\n\\n\"Yeah, yeah, yeah, yeah. Actually, I\\'m pretty good at cricket. I love it. I got the grades in Math in my college, but never really got up to'},\n {'generated_text': 'Hello, I like to play cricket, I like to play cricket in England, I love India. But I don\\'t understand why cricket is a sport I can\\'t play to\".\\n\\nSohil, whose house in Rajasthan has been given the state seal, has no other choice but'},\n {'generated_text': 'Hello, I like to play cricket, I like to play football and I like to love cricket and I want to spend my time on it.\"\\n\\nHe played a day and a half after his first game, and the only thing that remained was playing his poor team-mates, who were in'},\n {'generated_text': 'Hello, I like to play cricket, I guess. Do you know how many batsmen have ever walked onto the field? Five, because they have. That\\'s what I did.\"\\n\\nA few years ago after finishing third in the 2011 Test rankings, he told reporters: \"The only way'}]"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### English to German translation","metadata":{}},{"cell_type":"code","source":"# English to German\ntranslator_ger = pipeline(\"translation_en_to_de\")\nprint(\"German: \",translator_ger(\"We can make it together\", max_length=40)[0]['translation_text'])","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:56:20.392674Z","iopub.execute_input":"2023-11-07T13:56:20.393107Z","iopub.status.idle":"2023-11-07T13:56:24.224082Z","shell.execute_reply.started":"2023-11-07T13:56:20.393076Z","shell.execute_reply":"2023-11-07T13:56:24.223205Z"},"trusted":true},"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"name":"stdout","text":"German:  Wir können es gemeinsam machen\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Ukrainian to English translator","metadata":{}},{"cell_type":"code","source":"translator_ger = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-zle-en\")\nprint(translator_ger(\"Мене звати Вольфґанґ і я живу в Берліні.\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-07T13:57:42.912276Z","iopub.execute_input":"2023-11-07T13:57:42.912645Z","iopub.status.idle":"2023-11-07T13:58:05.777616Z","shell.execute_reply.started":"2023-11-07T13:57:42.912614Z","shell.execute_reply":"2023-11-07T13:58:05.775434Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c6132e7eae422a8b8dbc53cf1b8afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/478M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8248f5ac3340e5929db605dad9b9f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52153ffe05347e89f42e65d8d63d778"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89713d1b07ac4369886a884b38235acb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading source.spm:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad1bb2ae2efd43bab86c35fd2c31a5d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45e6cee27ca04a63a66fd18f9f65c926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/2.49M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e6817a68a74ae28c463271c836d999"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a117dcc43abb408590a7f8fe27136bca"}},"metadata":{}},{"name":"stdout","text":"[{'translation_text': 'My name is Wolfgang and I live in Berlin.'}]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Translation using T5\n","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\ninput_ids = tokenizer(\"We can make it together\", return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:07:28.749453Z","iopub.execute_input":"2023-11-07T14:07:28.749826Z","iopub.status.idle":"2023-11-07T14:07:30.070008Z","shell.execute_reply.started":"2023-11-07T14:07:28.749799Z","shell.execute_reply":"2023-11-07T14:07:30.069069Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Wir können es gemeinsam schaffen.\n","output_type":"stream"}],"execution_count":22}]}